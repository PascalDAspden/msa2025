{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2249078461bec40c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 0. Folder Structure\n",
    "```\n",
    "3. Deep Learning Project/\n",
    "├── dataset_split/\n",
    "│   ├── train/\n",
    "│   │   ├── african_elephant (780 images)\n",
    "│   │   ├── airliner (780 images)\n",
    "│   │   └── ... (8 more folders)\n",
    "│   ├── val/\n",
    "│   │   ├── african_elephant (260 images)\n",
    "│   │   ├── airliner (260 images)\n",
    "│   │   └── ... (8 more folders)\n",
    "│   └── test (2600 images)\n",
    "├── data_preprocessed/\n",
    "│   ├── labels_train.pt\n",
    "│   ├── labels_val.pt\n",
    "│   ├── tensor_test.pt\n",
    "│   ├── tensor_train.pt\n",
    "│   └── tensor_val.pt\n",
    "├── models/\n",
    "│   ├── resnet18_checkpoint.pkl\n",
    "│   └── resnet34_checkpoint.pkl\n",
    "├── src/\n",
    "│   └── part3.ipynb\n",
    "├── src_datasplit/\n",
    "│   └── data_split.ipynb\n",
    "├── MSA.yaml\n",
    "├── README.md\n",
    "└── IEEE_Report_Template.docx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff9f82a794ed57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:43:05.563885Z",
     "start_time": "2025-05-10T03:43:05.550616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129404feccd45358",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1 Preprocessing\n",
    "1. Load images and labels from folders into lists.\n",
    "2. Resize the images.\n",
    "3. Convert the image data type as float32 and limit the range in [0, 1].\n",
    "4. Convert data from NHWC to NCHW and creat a PyTorch tensor from it.\n",
    "5. Per-channel standardization of the dataset using training set statistics (mean and std computed independently for each color channel).\n",
    "6. Saving the pre-processed sets as .pt files in the correct directory.\n",
    "7. Load the pre-processed sets from .pt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:13.117713Z",
     "start_time": "2025-05-10T03:38:13.110009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def image_loader(set_dir: str) -> Tuple[List[np.ndarray], List[str]]:\n",
    "    \"\"\"\n",
    "    Load all images from a dataset directory (train/val/test) into memory.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Check if it's a test set (i.e., no subdirectories)\n",
    "    if all(os.path.isfile(os.path.join(set_dir, f)) for f in os.listdir(set_dir)):\n",
    "        # Test set: load files in sorted order by filename\n",
    "        file_names = sorted(os.listdir(set_dir), key=lambda x: int(os.path.splitext(x)[0]))\n",
    "        for fname in file_names:\n",
    "            img_path = os.path.join(set_dir, fname)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        labels = []  # test set has no labels\n",
    "\n",
    "    else:\n",
    "        # Train/val set: subfolders = class names\n",
    "        for class_name in sorted(os.listdir(set_dir)):\n",
    "            class_dir = os.path.join(set_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for fname in sorted(os.listdir(class_dir)):\n",
    "                    img_path = os.path.join(class_dir, fname)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        images.append(img)\n",
    "                        labels.append(class_name)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9151a560c344fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:13.472357Z",
     "start_time": "2025-05-10T03:38:13.460924Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def img_resize(images: List[np.ndarray], target_size: int = 224) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize images with the shortest edge to target_size and perform center cropping to square dimensions.\n",
    "    \"\"\"\n",
    "    if not images:\n",
    "        return np.array([])\n",
    "\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        if img is None or len(img.shape) != 3 or img.shape[2] != 3:\n",
    "            raise ValueError(\"Each image must be in HWC format with 3 channels (BGR).\")\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Compute scale factor to resize shortest side to target_size\n",
    "        scale = target_size / min(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "\n",
    "        # Resize with preserved aspect ratio\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Center crop to target_size x target_size\n",
    "        start_x = (new_w - target_size) // 2\n",
    "        start_y = (new_h - target_size) // 2\n",
    "        cropped = resized[start_y:start_y + target_size, start_x:start_x + target_size]\n",
    "\n",
    "        processed_images.append(cropped)\n",
    "\n",
    "    return np.stack(processed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1862b268144556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:39.249145Z",
     "start_time": "2025-05-10T03:38:14.305937Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "\n",
    "images_train, labels_train = image_loader(\"../dataset_split/train\")\n",
    "images_val, labels_val = image_loader(\"../dataset_split/val\")\n",
    "images_test, _ = image_loader(\"../dataset_split/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975435fea13d751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:17.095684Z",
     "start_time": "2025-05-10T03:38:57.883480Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Resize the images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m images_train = img_resize(images_train, \u001b[32m224\u001b[39m)\n\u001b[32m      4\u001b[39m images_val = img_resize(images_val, \u001b[32m224\u001b[39m)\n\u001b[32m      5\u001b[39m images_test = img_resize(images_test, \u001b[32m224\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mimg_resize\u001b[39m\u001b[34m(images, target_size)\u001b[39m\n\u001b[32m     25\u001b[39m     cropped = resized[start_y:start_y + target_size, start_x:start_x + target_size]\n\u001b[32m     27\u001b[39m     processed_images.append(cropped)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.stack(processed_images)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MSA/lib/python3.12/site-packages/numpy/core/shape_base.py:449\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    447\u001b[39m shapes = {arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mall input arrays must have the same shape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    451\u001b[39m result_ndim = arrays[\u001b[32m0\u001b[39m].ndim + \u001b[32m1\u001b[39m\n\u001b[32m    452\u001b[39m axis = normalize_axis_index(axis, result_ndim)\n",
      "\u001b[31mValueError\u001b[39m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "# Resize the images\n",
    "\n",
    "images_train = img_resize(images_train, 224)\n",
    "images_val = img_resize(images_val, 224)\n",
    "images_test = img_resize(images_test, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71fcddecfefdbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:20.788788Z",
     "start_time": "2025-05-10T03:39:17.097418Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: 3. Convert the image data type as float32 and limit the range in [0, 1].\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m images_train = images_train.astype(np.float32) / \u001b[32m255.0\u001b[39m\n\u001b[32m      4\u001b[39m images_val = images_val.astype(np.float32) / \u001b[32m255.0\u001b[39m\n\u001b[32m      5\u001b[39m images_test = images_test.astype(np.float32) / \u001b[32m255.0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# TODO: 3. Convert the image data type as float32 and limit the range in [0, 1].\n",
    "\n",
    "images_train = images_train.astype(np.float32) / 255.0\n",
    "images_val = images_val.astype(np.float32) / 255.0\n",
    "images_test = images_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938cec6018a8176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:21.156559Z",
     "start_time": "2025-05-10T03:39:20.789492Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 4. Convert data from NHWC to NCHW and creat a PyTorch tensor from it.\n",
    "\n",
    "tensor_train = torch.from_numpy(images_train).permute(0, 3, 1, 2).float()\n",
    "tensor_val = torch.from_numpy(images_val).permute(0, 3, 1, 2).float()\n",
    "tensor_test = torch.from_numpy(images_test).permute(0, 3, 1, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584e1b61e015747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:32.523629Z",
     "start_time": "2025-05-10T03:39:21.158863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Per-channel standardization of the dataset using training set statistics (mean and std computed independently for each color channel).\n",
    "\n",
    "train_mean = tensor_train.mean(dim=(0, 2, 3))\n",
    "train_std = tensor_train.std(dim=(0, 2, 3))\n",
    "\n",
    "tensor_train = (tensor_train - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n",
    "tensor_val = (tensor_val - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n",
    "tensor_test = (tensor_test - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a684ba9e6561c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:41.944582Z",
     "start_time": "2025-05-10T03:39:32.524747Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 6. Save the pre-processed sets as .pt files.\n",
    "\n",
    "# 6.1 Save the preprocessed tensors\n",
    "torch.save(tensor_train, \"tensor_train.pt\")\n",
    "torch.save(tensor_val, \"tensor_val.pt\")\n",
    "torch.save(tensor_test, \"tensor_test.pt\")\n",
    "\n",
    "# 6.2 Create unique label list and mappings\n",
    "unique_labels = sorted(set(labels_train))  # assuming labels_train was loaded earlier\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "# 6.3 Map string labels to integer indices\n",
    "labels_train_index = [label_to_index[label] for label in labels_train]\n",
    "labels_val_index = [label_to_index[label] for label in labels_val]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "labels_train_index = torch.tensor(labels_train_index, dtype=torch.long)\n",
    "labels_val_index = torch.tensor(labels_val_index, dtype=torch.long)\n",
    "\n",
    "# 6.4 Save label-related mappings and indices\n",
    "torch.save(labels_train_index, \"labels_train_index.pt\")\n",
    "torch.save(labels_val_index, \"labels_val_index.pt\")\n",
    "torch.save(label_to_index, \"label_to_index.pt\")\n",
    "torch.save(index_to_label, \"index_to_label.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5a646027a826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:45.192760Z",
     "start_time": "2025-05-10T03:39:41.946771Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 7. Load the pre-processed sets from .pt files. The data can be used directly in future without pre-processing once more by loading from the folder.\n",
    "\n",
    "tensor_train = torch.load(\"tensor_train.pt\")\n",
    "tensor_val = torch.load(\"tensor_val.pt\")\n",
    "tensor_test = torch.load(\"tensor_test.pt\")\n",
    "\n",
    "labels_train_index = torch.load(\"labels_train_index.pt\")\n",
    "labels_val_index = torch.load(\"labels_val_index.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb7cfa317410ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 2 Train the teacher model (ResNet34)\n",
    "1. Set hyperparameters for the training process.\n",
    "2. Create a pretrained ResNet34.\n",
    "3. Define the optimizer, loss function, etc.\n",
    "4. Train ResNet34 and save the best-performing parameters to 'resnet34_checkpoint.pkl'.\n",
    "5. Plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745979a1b12465eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:51.705549Z",
     "start_time": "2025-05-10T01:43:51.663106Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify data shapes\n",
    "\n",
    "num_classes = max(labels_train_index) + 1\n",
    "print(num_classes)\n",
    "print(tensor_train.shape)\n",
    "print(tensor_val.shape)\n",
    "print(tensor_test.shape)\n",
    "print(labels_train_index.shape)\n",
    "print(labels_val_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507aa43e1a3701b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:52.604546Z",
     "start_time": "2025-05-10T01:43:52.600245Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. Set hyperparameters for the training process.\n",
    "\n",
    "MAX_EPOCH = \n",
    "INIT_LR = \n",
    "BATCH_SIZE = \n",
    "\n",
    "# Specify the training device (automatically detects GPU if available)\n",
    "# Usage example: \n",
    "#   tensor = tensor.to(device)  # Moves tensor to selected device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9bf9ed5ae5cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:55.541802Z",
     "start_time": "2025-05-10T01:43:55.291383Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 2. Create a pretrained ResNet34.\n",
    "\n",
    "# 2.1 Use torchvision to create a pretrained ResNet34.\n",
    "resnet34 = \n",
    "\n",
    "# 2.2 Modify the output layer so that the model aligns with this project.\n",
    "\n",
    "# 2.2 Move the model to selected device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de5ac59beb12d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:56.555839Z",
     "start_time": "2025-05-10T01:43:56.542721Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Define the optimizer, loss function, etc.\n",
    "\n",
    "optimizer = \n",
    "criterion = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a797debcc531db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 4. Train ResNet34 and save the best-performing parameters to 'resnet34_checkpoint.pkl'.\n",
    "# Hint: May not need too many epochs as this is a pretrained model.\n",
    "\n",
    "train_accs_resnet34, train_losses_resnet34, val_accs_resnet34, val_losses_resnet34 = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a3089891cf4a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Plot the loss and accuracy curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fa4102b57deab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 3 Train the student model (ResNet18)\n",
    "1. Set hyperparameters for the distillation process.\n",
    "2. Create a NON-PRETRAINED ResNet18 and move the model to selected device.\n",
    "3. Load the saved ResNet34 and move the model to selected device.\n",
    "4. Define the optimizer, loss function, etc.\n",
    "5. Train ResNet18 and save the best-performing parameters to 'resnet18_checkpoint.pkl'.\n",
    "6. Plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68482456ab9410f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:38.370616Z",
     "start_time": "2025-05-10T01:57:38.341026Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify data shapes\n",
    "\n",
    "num_classes = max(labels_train_index) + 1\n",
    "print(num_classes)\n",
    "print(tensor_train.shape)\n",
    "print(tensor_val.shape)\n",
    "print(tensor_test.shape)\n",
    "print(labels_train_index.shape)\n",
    "print(labels_val_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a405975e022459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:39.329470Z",
     "start_time": "2025-05-10T01:57:39.294551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. Set hyperparameters for the distillation process.\n",
    "\n",
    "MAX_EPOCH =\n",
    "INIT_LR =\n",
    "BATCH_SIZE =\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEMPERATURE = \n",
    "LOSS_RATIO = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2325836c2cf7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:43.030265Z",
     "start_time": "2025-05-10T01:57:42.861042Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 2. Create a NON-PRETRAINED ResNet18 and move the model to selected device.\n",
    "\n",
    "resnet18 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8092f5d5acce972",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Load the saved ResNet34 and move the model to selected device.\n",
    "# Hint: Enable the evaluation mode to prevent updating the parameters.\n",
    "\n",
    "resnet34 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312a04da1e5ddf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:49.170629Z",
     "start_time": "2025-05-10T01:57:49.152278Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 4. Define the optimizer, loss function, etc.\n",
    "\n",
    "optimizer = \n",
    "scheduler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17f3f9eb662cef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Train ResNet18 and save the best-performing parameters to 'resnet18_checkpoint.pkl'.\n",
    "# Hint: Use the correct loss function for knowledge distillation.\n",
    "#       Details can be fund in https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html.\n",
    "\n",
    "train_accs_resnet18, train_losses_resnet18, val_accs_resnet18, val_losses_resnet18 = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260396b6b143a422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:04:33.157515Z",
     "start_time": "2025-05-10T03:04:32.960493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 6. Plot the loss and accuracy curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730790a6c4e96912",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 4 Predict labels in the testing set\n",
    "1. Load the saved ResNet18 and move it to the selected device.\n",
    "2. Do prediction using the images in the testing set.\n",
    "3. Write the results into the CSV file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8f6f703b42a0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. Load the saved ResNet18 and move it to the selected device.\n",
    "# Hint: Enable the evaluation mode to prevent updating the parameters.\n",
    "\n",
    "resnet18 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee908450e937e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:43:20.873625Z",
     "start_time": "2025-05-10T03:43:18.999813Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 2. Do prediction using the images in the testing set.\n",
    "predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5ddbde0d132cf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Write the results into the CSV file for submission.\n",
    "# Hint: The CSV file should be in the format of 'file_name' and 'label'.\n",
    "#       The submission.csv should look like:\n",
    "#           file_name,label\n",
    "#           0.jpg,sunglasses\n",
    "#           ...\n",
    "\n",
    "index_to_label = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
