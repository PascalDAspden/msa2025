{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2249078461bec40c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 0. Folder Structure\n",
    "```\n",
    "3. Deep Learning Project/\n",
    "├── dataset_split/\n",
    "│   ├── train/\n",
    "│   │   ├── african_elephant (780 images)\n",
    "│   │   ├── airliner (780 images)\n",
    "│   │   └── ... (8 more folders)\n",
    "│   ├── val/\n",
    "│   │   ├── african_elephant (260 images)\n",
    "│   │   ├── airliner (260 images)\n",
    "│   │   └── ... (8 more folders)\n",
    "│   └── test (2600 images)\n",
    "├── data_preprocessed/\n",
    "│   ├── labels_train.pt\n",
    "│   ├── labels_val.pt\n",
    "│   ├── tensor_test.pt\n",
    "│   ├── tensor_train.pt\n",
    "│   └── tensor_val.pt\n",
    "├── models/\n",
    "│   ├── resnet18_checkpoint.pkl\n",
    "│   └── resnet34_checkpoint.pkl\n",
    "├── src/\n",
    "│   └── part3.ipynb\n",
    "├── src_datasplit/\n",
    "│   └── data_split.ipynb\n",
    "├── MSA.yaml\n",
    "├── README.md\n",
    "└── IEEE_Report_Template.docx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff9f82a794ed57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:43:05.563885Z",
     "start_time": "2025-05-10T03:43:05.550616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129404feccd45358",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1 Preprocessing\n",
    "1. Load images and labels from folders into lists.\n",
    "2. Resize the images.\n",
    "3. Convert the image data type as float32 and limit the range in [0, 1].\n",
    "4. Convert data from NHWC to NCHW and creat a PyTorch tensor from it.\n",
    "5. Per-channel standardization of the dataset using training set statistics (mean and std computed independently for each color channel).\n",
    "6. Saving the pre-processed sets as .pt files in the correct directory.\n",
    "7. Load the pre-processed sets from .pt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:13.117713Z",
     "start_time": "2025-05-10T03:38:13.110009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_loader(set_dir: str) -> Tuple[List[np.ndarray], List[str]]:\n",
    "    \"\"\"\n",
    "    Load all images from a dataset directory (train/val/test) into memory.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Check if it's a test set (i.e., no subdirectories)\n",
    "    if all(os.path.isfile(os.path.join(set_dir, f)) for f in os.listdir(set_dir)):\n",
    "        # Test set: load files in sorted order by filename\n",
    "        file_names = sorted(os.listdir(set_dir), key=lambda x: int(os.path.splitext(x)[0]))\n",
    "        for fname in file_names:\n",
    "            img_path = os.path.join(set_dir, fname)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        labels = []  # test set has no labels\n",
    "\n",
    "    else:\n",
    "        # Train/val set: subfolders = class names\n",
    "        for class_name in sorted(os.listdir(set_dir)):\n",
    "            class_dir = os.path.join(set_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for fname in sorted(os.listdir(class_dir)):\n",
    "                    img_path = os.path.join(class_dir, fname)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        images.append(img)\n",
    "                        labels.append(class_name)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9151a560c344fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:13.472357Z",
     "start_time": "2025-05-10T03:38:13.460924Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def img_resize(images: List[np.ndarray], target_size: int = 224) -> np.ndarray:\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        h, w = img.shape[:2]\n",
    "        scale = target_size / min(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Calculate crop start with floor\n",
    "        start_x = max((new_w - target_size) // 2, 0)\n",
    "        start_y = max((new_h - target_size) // 2, 0)\n",
    "        cropped = resized[start_y:start_y + target_size, start_x:start_x + target_size]\n",
    "\n",
    "        # Pad if cropped too small (due to rounding edge-case)\n",
    "        pad_h = target_size - cropped.shape[0]\n",
    "        pad_w = target_size - cropped.shape[1]\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            cropped = cv2.copyMakeBorder(\n",
    "                cropped,\n",
    "                0, pad_h, 0, pad_w,\n",
    "                borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "            )\n",
    "\n",
    "        assert cropped.shape == (target_size, target_size, 3), f\"Bad shape: {cropped.shape}\"\n",
    "        processed.append(cropped)\n",
    "\n",
    "    return np.stack(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1862b268144556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:38:39.249145Z",
     "start_time": "2025-05-10T03:38:14.305937Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "\n",
    "images_train, labels_train = image_loader(\"../dataset_split/train\")\n",
    "images_val, labels_val = image_loader(\"../dataset_split/val\")\n",
    "images_test, _ = image_loader(\"../dataset_split/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975435fea13d751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:17.095684Z",
     "start_time": "2025-05-10T03:38:57.883480Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resize the images\n",
    "\n",
    "images_train = img_resize(images_train, 224)\n",
    "images_val = img_resize(images_val, 224)\n",
    "images_test = img_resize(images_test, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71fcddecfefdbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:20.788788Z",
     "start_time": "2025-05-10T03:39:17.097418Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Convert the image data type as float32 and limit the range in [0, 1].\n",
    "\n",
    "images_train = images_train.astype(np.float32) / 255.0\n",
    "images_val = images_val.astype(np.float32) / 255.0\n",
    "images_test = images_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1938cec6018a8176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:21.156559Z",
     "start_time": "2025-05-10T03:39:20.789492Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 4. Convert data from NHWC to NCHW and creat a PyTorch tensor from it.\n",
    "\n",
    "tensor_train = torch.from_numpy(images_train).permute(0, 3, 1, 2).float()\n",
    "tensor_val = torch.from_numpy(images_val).permute(0, 3, 1, 2).float()\n",
    "tensor_test = torch.from_numpy(images_test).permute(0, 3, 1, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d584e1b61e015747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:32.523629Z",
     "start_time": "2025-05-10T03:39:21.158863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Per-channel standardization of the dataset using training set statistics (mean and std computed independently for each color channel).\n",
    "\n",
    "train_mean = tensor_train.mean(dim=(0, 2, 3))\n",
    "train_std = tensor_train.std(dim=(0, 2, 3))\n",
    "\n",
    "tensor_train = (tensor_train - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n",
    "tensor_val = (tensor_val - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n",
    "tensor_test = (tensor_test - train_mean[None, :, None, None]) / train_std[None, :, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7a684ba9e6561c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:41.944582Z",
     "start_time": "2025-05-10T03:39:32.524747Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 6. Save the pre-processed sets as .pt files.\n",
    "\n",
    "# 6.1 Save the preprocessed tensors\n",
    "torch.save(tensor_train, \"tensor_train.pt\")\n",
    "torch.save(tensor_val, \"tensor_val.pt\")\n",
    "torch.save(tensor_test, \"tensor_test.pt\")\n",
    "\n",
    "# 6.2 Create unique label list and mappings\n",
    "unique_labels = sorted(set(labels_train))  # assuming labels_train was loaded earlier\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "# 6.3 Map string labels to integer indices\n",
    "labels_train_index = [label_to_index[label] for label in labels_train]\n",
    "labels_val_index = [label_to_index[label] for label in labels_val]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "labels_train_index = torch.tensor(labels_train_index, dtype=torch.long)\n",
    "labels_val_index = torch.tensor(labels_val_index, dtype=torch.long)\n",
    "\n",
    "# 6.4 Save label-related mappings and indices\n",
    "torch.save(labels_train_index, \"labels_train_index.pt\")\n",
    "torch.save(labels_val_index, \"labels_val_index.pt\")\n",
    "torch.save(label_to_index, \"label_to_index.pt\")\n",
    "torch.save(index_to_label, \"index_to_label.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a5a646027a826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:39:45.192760Z",
     "start_time": "2025-05-10T03:39:41.946771Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/22ly48k565557xrbtq5p2dm80000gn/T/ipykernel_6968/295508795.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor_train = torch.load(\"tensor_train.pt\")\n",
      "/var/folders/xk/22ly48k565557xrbtq5p2dm80000gn/T/ipykernel_6968/295508795.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor_val = torch.load(\"tensor_val.pt\")\n",
      "/var/folders/xk/22ly48k565557xrbtq5p2dm80000gn/T/ipykernel_6968/295508795.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor_test = torch.load(\"tensor_test.pt\")\n",
      "/var/folders/xk/22ly48k565557xrbtq5p2dm80000gn/T/ipykernel_6968/295508795.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_train_index = torch.load(\"labels_train_index.pt\")\n",
      "/var/folders/xk/22ly48k565557xrbtq5p2dm80000gn/T/ipykernel_6968/295508795.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_val_index = torch.load(\"labels_val_index.pt\")\n"
     ]
    }
   ],
   "source": [
    "# TODO: 7. Load the pre-processed sets from .pt files. The data can be used directly in future without pre-processing once more by loading from the folder.\n",
    "\n",
    "tensor_train = torch.load(\"tensor_train.pt\")\n",
    "tensor_val = torch.load(\"tensor_val.pt\")\n",
    "tensor_test = torch.load(\"tensor_test.pt\")\n",
    "\n",
    "labels_train_index = torch.load(\"labels_train_index.pt\")\n",
    "labels_val_index = torch.load(\"labels_val_index.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb7cfa317410ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 2 Train the teacher model (ResNet34)\n",
    "1. Set hyperparameters for the training process.\n",
    "2. Create a pretrained ResNet34.\n",
    "3. Define the optimizer, loss function, etc.\n",
    "4. Train ResNet34 and save the best-performing parameters to 'resnet34_checkpoint.pkl'.\n",
    "5. Plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745979a1b12465eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:51.705549Z",
     "start_time": "2025-05-10T01:43:51.663106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n",
      "torch.Size([7800, 3, 224, 224])\n",
      "torch.Size([2600, 3, 224, 224])\n",
      "torch.Size([2600, 3, 224, 224])\n",
      "torch.Size([7800])\n",
      "torch.Size([2600])\n"
     ]
    }
   ],
   "source": [
    "# Verify data shapes\n",
    "\n",
    "num_classes = max(labels_train_index) + 1\n",
    "print(num_classes)\n",
    "print(tensor_train.shape)\n",
    "print(tensor_val.shape)\n",
    "print(tensor_test.shape)\n",
    "print(labels_train_index.shape)\n",
    "print(labels_val_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507aa43e1a3701b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:52.604546Z",
     "start_time": "2025-05-10T01:43:52.600245Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_EPOCH = 5  # Number of epochs to train the model\n",
    "INIT_LR = 1e-3  # Initial learning rate\n",
    "BATCH_SIZE = 64  # Number of samples per batch\n",
    "\n",
    "\n",
    "# Specify the training device (automatically detects GPU if available)\n",
    "# Usage example: \n",
    "#   tensor = tensor.to(device)  # Moves tensor to selected device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b9bf9ed5ae5cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:55.541802Z",
     "start_time": "2025-05-10T01:43:55.291383Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 2. Create a pretrained ResNet34.\n",
    "\n",
    "# Load the pretrained ResNet-34 model\n",
    "resnet34 = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Define the number of output classes (adjust as per your dataset)\n",
    "num_classes = 10\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes\n",
    "resnet34.fc = nn.Linear(resnet34.fc.in_features, num_classes)\n",
    "\n",
    "# Specify the training device (automatically detects GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "resnet34.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70de5ac59beb12d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:43:56.555839Z",
     "start_time": "2025-05-10T01:43:56.542721Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Define the optimizer, loss function, etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet34.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a797debcc531db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils.data' has no attribute 'train_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m running_corrects = \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m torch.utils.data.train_loader:\n\u001b[32m     17\u001b[39m     inputs = inputs.to(device)\n\u001b[32m     18\u001b[39m     labels = labels.to(device)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.utils.data' has no attribute 'train_loader'"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(resnet34.state_dict())\n",
    "\n",
    "train_accs_resnet34, train_losses_resnet34 = [], []\n",
    "val_accs_resnet34, val_losses_resnet34 = [], []\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    print(f'Epoch {epoch+1}/{MAX_EPOCH}')\n",
    "    \n",
    "    # Training phase\n",
    "    resnet34.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in torch.utils.data.train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = resnet34(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects.double() / total\n",
    "    train_losses_resnet34.append(epoch_loss)\n",
    "    train_accs_resnet34.append(epoch_acc.item())\n",
    "    \n",
    "    # Validation phase\n",
    "    resnet34.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in torch.utils.data.val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = resnet34(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_loss = val_running_loss / val_total\n",
    "    val_acc = val_running_corrects.double() / val_total\n",
    "    val_losses_resnet34.append(val_loss)\n",
    "    val_accs_resnet34.append(val_acc.item())\n",
    "    \n",
    "    print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Save best model weights\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(resnet34.state_dict())\n",
    "        torch.save(best_model_wts, 'resnet34_checkpoint.pkl')\n",
    "\n",
    "# Load best model weights after training\n",
    "resnet34.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a3089891cf4a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Plot the loss and accuracy curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fa4102b57deab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 3 Train the student model (ResNet18)\n",
    "1. Set hyperparameters for the distillation process.\n",
    "2. Create a NON-PRETRAINED ResNet18 and move the model to selected device.\n",
    "3. Load the saved ResNet34 and move the model to selected device.\n",
    "4. Define the optimizer, loss function, etc.\n",
    "5. Train ResNet18 and save the best-performing parameters to 'resnet18_checkpoint.pkl'.\n",
    "6. Plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68482456ab9410f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:38.370616Z",
     "start_time": "2025-05-10T01:57:38.341026Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify data shapes\n",
    "\n",
    "num_classes = max(labels_train_index) + 1\n",
    "print(num_classes)\n",
    "print(tensor_train.shape)\n",
    "print(tensor_val.shape)\n",
    "print(tensor_test.shape)\n",
    "print(labels_train_index.shape)\n",
    "print(labels_val_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a405975e022459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:39.329470Z",
     "start_time": "2025-05-10T01:57:39.294551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. Set hyperparameters for the distillation process.\n",
    "\n",
    "MAX_EPOCH =\n",
    "INIT_LR =\n",
    "BATCH_SIZE =\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEMPERATURE = \n",
    "LOSS_RATIO = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2325836c2cf7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:43.030265Z",
     "start_time": "2025-05-10T01:57:42.861042Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 2. Create a NON-PRETRAINED ResNet18 and move the model to selected device.\n",
    "\n",
    "resnet18 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8092f5d5acce972",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Load the saved ResNet34 and move the model to selected device.\n",
    "# Hint: Enable the evaluation mode to prevent updating the parameters.\n",
    "\n",
    "resnet34 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312a04da1e5ddf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:57:49.170629Z",
     "start_time": "2025-05-10T01:57:49.152278Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 4. Define the optimizer, loss function, etc.\n",
    "\n",
    "optimizer = \n",
    "scheduler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17f3f9eb662cef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 5. Train ResNet18 and save the best-performing parameters to 'resnet18_checkpoint.pkl'.\n",
    "# Hint: Use the correct loss function for knowledge distillation.\n",
    "#       Details can be fund in https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html.\n",
    "\n",
    "train_accs_resnet18, train_losses_resnet18, val_accs_resnet18, val_losses_resnet18 = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260396b6b143a422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:04:33.157515Z",
     "start_time": "2025-05-10T03:04:32.960493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 6. Plot the loss and accuracy curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730790a6c4e96912",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 4 Predict labels in the testing set\n",
    "1. Load the saved ResNet18 and move it to the selected device.\n",
    "2. Do prediction using the images in the testing set.\n",
    "3. Write the results into the CSV file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8f6f703b42a0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. Load the saved ResNet18 and move it to the selected device.\n",
    "# Hint: Enable the evaluation mode to prevent updating the parameters.\n",
    "\n",
    "resnet18 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee908450e937e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:43:20.873625Z",
     "start_time": "2025-05-10T03:43:18.999813Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 2. Do prediction using the images in the testing set.\n",
    "predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5ddbde0d132cf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 3. Write the results into the CSV file for submission.\n",
    "# Hint: The CSV file should be in the format of 'file_name' and 'label'.\n",
    "#       The submission.csv should look like:\n",
    "#           file_name,label\n",
    "#           0.jpg,sunglasses\n",
    "#           ...\n",
    "\n",
    "index_to_label = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
